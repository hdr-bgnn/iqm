{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from HGNN.train.configParser import ConfigParser\n",
    "from HGNN.train import CNN, dataLoader\n",
    "\n",
    "experimentsFileName = \"experiments.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath=\"/home/jnl47/Documents/dev/HGNN/experiments/\"\n",
    "dataPath=\"/home/jnl47/Documents/dev/HGNN/data\"\n",
    "experimentName=\"smegma\"\n",
    "trial_hash=\"06feff02666707cbdecf12fdbec650a36bef56a47085a37a2ddf1515\"\n",
    "\n",
    "cuda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 1\n"
     ]
    }
   ],
   "source": [
    "# set cuda\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(cuda)\n",
    "    print(\"using cuda\", cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experimentName</th>\n",
       "      <th>modelName</th>\n",
       "      <th>datasetName</th>\n",
       "      <th>experimentHash</th>\n",
       "      <th>trialHash</th>\n",
       "      <th>image_path</th>\n",
       "      <th>suffix</th>\n",
       "      <th>training_count</th>\n",
       "      <th>validation_count</th>\n",
       "      <th>batchSize</th>\n",
       "      <th>...</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>numOfTrials</th>\n",
       "      <th>patience</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>fc_layers</th>\n",
       "      <th>modelType</th>\n",
       "      <th>lambda</th>\n",
       "      <th>unsupervisedOnTest</th>\n",
       "      <th>tl_model</th>\n",
       "      <th>augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smegma</td>\n",
       "      <td>models/06feff02666707cbdecf12fdbec650a36bef56a...</td>\n",
       "      <td>datasplits/0baedd7ad9e196e933c0628cd848466e306...</td>\n",
       "      <td>11cd54ba032786ffb6dda3320bd9466e9c5a8ee5903a0a...</td>\n",
       "      <td>06feff02666707cbdecf12fdbec650a36bef56a47085a3...</td>\n",
       "      <td>INHS_segmented_padded_fish</td>\n",
       "      <td>52</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.16</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>BB</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>ResNet18</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  experimentName                                          modelName  \\\n",
       "2         smegma  models/06feff02666707cbdecf12fdbec650a36bef56a...   \n",
       "\n",
       "                                         datasetName  \\\n",
       "2  datasplits/0baedd7ad9e196e933c0628cd848466e306...   \n",
       "\n",
       "                                      experimentHash  \\\n",
       "2  11cd54ba032786ffb6dda3320bd9466e9c5a8ee5903a0a...   \n",
       "\n",
       "                                           trialHash  \\\n",
       "2  06feff02666707cbdecf12fdbec650a36bef56a47085a3...   \n",
       "\n",
       "                   image_path  suffix  training_count  validation_count  \\\n",
       "2  INHS_segmented_padded_fish      52            0.64              0.16   \n",
       "\n",
       "   batchSize  ...  learning_rate  numOfTrials  patience  fc_width  fc_layers  \\\n",
       "2         32  ...         0.0001            5        50       200          1   \n",
       "\n",
       "   modelType lambda  unsupervisedOnTest  tl_model augmented  \n",
       "2         BB    0.6               False  ResNet18     False  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimentsFileName)\n",
    "if os.path.exists(experimentsFileNameAndPath):\n",
    "    experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "experiments_df[\"trialHash\"]\n",
    "experiments_df[experiments_df[\"trialHash\"] == trial_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/550 [00:00<?, ?it/s, fileName=INHS_FISH_68432.jpg]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'smegma', 'modelName': 'models/06feff02666707cbdecf12fdbec650a36bef56a47085a37a2ddf1515', 'datasetName': 'datasplits/0baedd7ad9e196e933c0628cd848466e3063d51e2319593293cab55c', 'experimentHash': '11cd54ba032786ffb6dda3320bd9466e9c5a8ee5903a0a9f7957c81b', 'trialHash': '06feff02666707cbdecf12fdbec650a36bef56a47085a37a2ddf1515', 'image_path': 'INHS_segmented_padded_fish', 'suffix': 52, 'training_count': 0.64, 'validation_count': 0.16, 'batchSize': 32, 'n_epochs': 5000, 'learning_rate': 0.0001, 'numOfTrials': 5, 'patience': 50, 'fc_width': 200, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0.6, 'unsupervisedOnTest': False, 'tl_model': 'ResNet18', 'augmented': False}\n",
      "Creating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 550/550 [00:56<00:00,  9.70it/s, fileName=INHS_FISH_92612.jpg] \n",
      "Transforming images:   2%|▏         | 9/550 [00:00<00:06, 85.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming images: 100%|██████████| 550/550 [00:09<00:00, 60.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test =  440 110\n",
      "train/val =  352 88\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n",
      "file /home/jnl47/Documents/dev/HGNN/experiments/smegma/datasplits/31a44652b8bc8fc1bea842ba67230348eefb450960e07f829f118faa/trainingIndex.csv written\n",
      "file /home/jnl47/Documents/dev/HGNN/experiments/smegma/datasplits/31a44652b8bc8fc1bea842ba67230348eefb450960e07f829f118faa/valIndex.csv written\n",
      "file /home/jnl47/Documents/dev/HGNN/experiments/smegma/datasplits/31a44652b8bc8fc1bea842ba67230348eefb450960e07f829f118faa/testIndex.csv written\n"
     ]
    }
   ],
   "source": [
    "# Get experiment parameters\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimentsFileName)\n",
    "if os.path.exists(experimentsFileNameAndPath):\n",
    "    experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "else:\n",
    "    raise Exception(\"Experiment not \" + trial_hash + \" found!\")\n",
    "experimentRecord = experiments_df[experiments_df[\"trialHash\"] == trial_hash]\n",
    "experiment_params = experimentRecord.to_dict('records')[0]\n",
    "print(experiment_params)\n",
    "\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "datasetManager = dataLoader.datasetManager(experimentPathAndName)\n",
    "datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "dataset = datasetManager.getDataset()\n",
    "train_loader, validation_loader, test_loader = datasetManager.getLoaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = {\n",
    "    \"fine\": len(dataset.csv_processor.getFineList()),\n",
    "    \"coarse\" : len(dataset.csv_processor.getCoarseList())\n",
    "}\n",
    "dataset.csv_processor.getFineList()\n",
    "model = CNN.create_model(architecture, experiment_params)\n",
    "model\n",
    "# get the model and the parameters\n",
    "modelName = experimentRecord.iloc[0][\"modelName\"]\n",
    "trialName = os.path.join(experimentPathAndName, modelName)\n",
    "_ = CNN.loadModel(model, trialName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lepomis auritus',\n",
       " 'Noturus eleutherus',\n",
       " 'Carassius auratus',\n",
       " 'Lepomis cyanellus',\n",
       " 'Esox americanus',\n",
       " 'Noturus insignis',\n",
       " 'Noturus flavus',\n",
       " 'Noturus gyrinus',\n",
       " 'Esox lucius',\n",
       " 'Noturus exilis',\n",
       " 'Cyprinus carpio']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.csv_processor.getFineList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_hierarchy(\n",
       "  (pretrained_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (custom_tl_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (9): Flatten()\n",
       "  )\n",
       "  (g_c): Sequential(\n",
       "    (linear0): Linear(in_features=200, out_features=5, bias=True)\n",
       "    (bnorm0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU()\n",
       "  )\n",
       "  (h_y): Sequential(\n",
       "    (linear0): Linear(in_features=512, out_features=200, bias=True)\n",
       "    (bnorm0): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU()\n",
       "  )\n",
       "  (g_y): Sequential(\n",
       "    (linear0): Linear(in_features=200, out_features=11, bias=True)\n",
       "    (bnorm0): BatchNorm1d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/06feff02666707cbdecf12fdbec650a36bef56a47085a37a2ddf1515'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort through predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_misclassified = pd.DataFrame(columns=['file name', 'true label', 'probability of true label', 'predicted label'])\n",
    "df_correctlyclassified = pd.DataFrame(columns=['file name', 'true label', 'probability of true label', 'predicted label'])\n",
    "\n",
    "# get probability of correct prediction and true label\n",
    "predProblist, lbllist = CNN.getLoaderPredictionProbabilities(test_loader, model, experiment_params)\n",
    "_, predlist = torch.max(predProblist, 1)\n",
    "lbllist = lbllist.reshape(lbllist.shape[0], -1)\n",
    "predProblist = predProblist.gather(1, lbllist)\n",
    "predProblist = predProblist.reshape(1, -1)\n",
    "predProblist = predProblist[0]\n",
    "\n",
    "# sort through\n",
    "predProblist, indices = torch.sort(predProblist)\n",
    "predlist = predlist[indices]\n",
    "lbllist = lbllist[indices]\n",
    "\n",
    "for i, lbl in enumerate(lbllist):\n",
    "    prd = predlist[i]\n",
    "    prdProb = predProblist[i]\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        lbl = lbl.cpu()\n",
    "        prd = prd.cpu()\n",
    "        prdProb = prdProb.cpu()\n",
    "\n",
    "    s = dataset[i]\n",
    "    row = {'file name' : s['fileName'] , \n",
    "           'true label' : int(lbl.numpy()), \n",
    "           'probability of true label': float(prdProb.numpy()),\n",
    "           'predicted label' : int(prd.numpy())}\n",
    "    \n",
    "    if(lbl != prd):\n",
    "        df_misclassified = df_misclassified.append(row, ignore_index=True)\n",
    "    else:\n",
    "        df_correctlyclassified = df_correctlyclassified.append(row, ignore_index=True)\n",
    "        \n",
    "df_misclassified = df_misclassified.sort_values(by=[ 'true label', 'probability of true label'])\n",
    "df_correctlyclassified = df_correctlyclassified.sort_values(by=['true label', 'probability of true label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display and save mispredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             file name true label  probability of true label predicted label\n",
      "0  INHS_FISH_68432.jpg          0                   0.029481               3\n",
      "1   INHS_FISH_2426.jpg          3                   0.032985              10\n",
      "4  INHS_FISH_95243.jpg          4                   0.179742               8\n",
      "2  INHS_FISH_93450.jpg          6                   0.053474               9\n",
      "3  INHS_FISH_89331.jpg          6                   0.057270               1\n"
     ]
    }
   ],
   "source": [
    "print(df_misclassified)\n",
    "df_misclassified.to_csv(os.path.join(experimentPathAndName, modelName, 'misclassified examples.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and display correctly predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filename of correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                file name true label  probability of true label  \\\n",
      "13    INHS_FISH_14454.jpg          0                   0.658075   \n",
      "15    INHS_FISH_76861.jpg          0                   0.693080   \n",
      "27    INHS_FISH_37733.jpg          0                   0.918069   \n",
      "29    INHS_FISH_41171.jpg          0                   0.933658   \n",
      "32   INHS_FISH_108140.jpg          0                   0.963959   \n",
      "..                    ...        ...                        ...   \n",
      "85    INHS_FISH_27267.jpg         10                   0.979372   \n",
      "97    INHS_FISH_33588.jpg         10                   0.981626   \n",
      "98    INHS_FISH_85801.jpg         10                   0.981755   \n",
      "101   INHS_FISH_83473.jpg         10                   0.983593   \n",
      "104  INHS_FISH_107230.jpg         10                   0.986648   \n",
      "\n",
      "    predicted label  \n",
      "13                0  \n",
      "15                0  \n",
      "27                0  \n",
      "29                0  \n",
      "32                0  \n",
      "..              ...  \n",
      "85               10  \n",
      "97               10  \n",
      "98               10  \n",
      "101              10  \n",
      "104              10  \n",
      "\n",
      "[105 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_correctlyclassified)\n",
    "df_correctlyclassified.to_csv(os.path.join(experimentPathAndName, modelName, 'correctly classified examples.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f439b526450>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
