{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine\n",
    "\n",
    "from HGNN.train.configParser import ConfigParser\n",
    "from HGNN.train import CNN, dataLoader\n",
    "\n",
    "experimentsFileName = \"experiments.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath=\"/home/jnl47/Documents/dev/HGNN/experiments/\"\n",
    "dataPath=\"/home/jnl47/Documents/dev/HGNN/data\"\n",
    "experimentName=\"allgraded\"\n",
    "trial_hash=\"cf5bb78c6fcc769776b2af4bf2d86d7be5835a7f9c4face14827d44f\"\n",
    "\n",
    "cuda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 1\n"
     ]
    }
   ],
   "source": [
    "# set cuda\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(cuda)\n",
    "    print(\"using cuda\", cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experimentName</th>\n",
       "      <th>modelName</th>\n",
       "      <th>datasetName</th>\n",
       "      <th>experimentHash</th>\n",
       "      <th>trialHash</th>\n",
       "      <th>image_path</th>\n",
       "      <th>suffix</th>\n",
       "      <th>training_count</th>\n",
       "      <th>validation_count</th>\n",
       "      <th>batchSize</th>\n",
       "      <th>...</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>numOfTrials</th>\n",
       "      <th>patience</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>fc_layers</th>\n",
       "      <th>modelType</th>\n",
       "      <th>lambda</th>\n",
       "      <th>unsupervisedOnTest</th>\n",
       "      <th>tl_model</th>\n",
       "      <th>augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>allgraded</td>\n",
       "      <td>models/cf5bb78c6fcc769776b2af4bf2d86d7be5835a7...</td>\n",
       "      <td>datasplits/4fc30fc78733889043b62f0c6b4fa76b373...</td>\n",
       "      <td>b6fa90f577bbf358bafbc9be0871fad9ffbc5b5d730248...</td>\n",
       "      <td>cf5bb78c6fcc769776b2af4bf2d86d7be5835a7f9c4fac...</td>\n",
       "      <td>INHS_segmented_padded_fish</td>\n",
       "      <td>allgraded</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.16</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>BB</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>ResNet18</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   experimentName                                          modelName  \\\n",
       "60      allgraded  models/cf5bb78c6fcc769776b2af4bf2d86d7be5835a7...   \n",
       "\n",
       "                                          datasetName  \\\n",
       "60  datasplits/4fc30fc78733889043b62f0c6b4fa76b373...   \n",
       "\n",
       "                                       experimentHash  \\\n",
       "60  b6fa90f577bbf358bafbc9be0871fad9ffbc5b5d730248...   \n",
       "\n",
       "                                            trialHash  \\\n",
       "60  cf5bb78c6fcc769776b2af4bf2d86d7be5835a7f9c4fac...   \n",
       "\n",
       "                    image_path     suffix  training_count  validation_count  \\\n",
       "60  INHS_segmented_padded_fish  allgraded            0.64              0.16   \n",
       "\n",
       "    batchSize  ...  learning_rate  numOfTrials  patience  fc_width  fc_layers  \\\n",
       "60         32  ...         0.0001            5        50       200          1   \n",
       "\n",
       "    modelType lambda  unsupervisedOnTest  tl_model augmented  \n",
       "60         BB    0.6               False  ResNet18     False  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimentsFileName)\n",
    "if os.path.exists(experimentsFileNameAndPath):\n",
    "    experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "experiments_df[\"trialHash\"]\n",
    "experiments_df[experiments_df[\"trialHash\"] == trial_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'allgraded', 'modelName': 'models/cf5bb78c6fcc769776b2af4bf2d86d7be5835a7f9c4face14827d44f', 'datasetName': 'datasplits/4fc30fc78733889043b62f0c6b4fa76b3733a07b1079ee11fb2eb148', 'experimentHash': 'b6fa90f577bbf358bafbc9be0871fad9ffbc5b5d730248e89ce7f4f2', 'trialHash': 'cf5bb78c6fcc769776b2af4bf2d86d7be5835a7f9c4face14827d44f', 'image_path': 'INHS_segmented_padded_fish', 'suffix': 'allgraded', 'training_count': 0.64, 'validation_count': 0.16, 'batchSize': 32, 'n_epochs': 5000, 'learning_rate': 0.0001, 'numOfTrials': 5, 'patience': 50, 'fc_width': 200, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0.6, 'unsupervisedOnTest': False, 'tl_model': 'ResNet18', 'augmented': False}\n",
      "Creating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 1703/1703 [02:17<00:00, 12.35it/s, fileName=INHS_FISH_97787.jpg] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "file /home/jnl47/Documents/dev/HGNN/experiments/allgraded/datasplits/b270091ac751885bcba217b9acffac4fdd2fe4f62950f52fd8930511/trainingIndex.csv read\n",
      "file /home/jnl47/Documents/dev/HGNN/experiments/allgraded/datasplits/b270091ac751885bcba217b9acffac4fdd2fe4f62950f52fd8930511/valIndex.csv read\n",
      "file /home/jnl47/Documents/dev/HGNN/experiments/allgraded/datasplits/b270091ac751885bcba217b9acffac4fdd2fe4f62950f52fd8930511/testIndex.csv read\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get experiment parameters\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimentsFileName)\n",
    "if os.path.exists(experimentsFileNameAndPath):\n",
    "    experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "else:\n",
    "    raise Exception(\"Experiment not \" + trial_hash + \" found!\")\n",
    "experimentRecord = experiments_df[experiments_df[\"trialHash\"] == trial_hash]\n",
    "experiment_params = experimentRecord.to_dict('records')[0]\n",
    "print(experiment_params)\n",
    "\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "datasetManager = dataLoader.datasetManager(experimentPathAndName)\n",
    "datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "dataset = datasetManager.getDataset()\n",
    "train_loader, validation_loader, test_loader = datasetManager.getLoaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = {\n",
    "    \"fine\": len(dataset.csv_processor.getFineList()),\n",
    "    \"coarse\" : len(dataset.csv_processor.getCoarseList())\n",
    "}\n",
    "dataset.csv_processor.getFineList()\n",
    "model = CNN.create_model(architecture, experiment_params)\n",
    "model\n",
    "# get the model and the parameters\n",
    "modelName = experimentRecord.iloc[0][\"modelName\"]\n",
    "trialName = os.path.join(experimentPathAndName, modelName)\n",
    "_ = CNN.loadModel(model, trialName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Notropis',\n",
       " 'Esox',\n",
       " 'Gambusia',\n",
       " 'Lepomis',\n",
       " 'Noturus',\n",
       " 'Phenacobius',\n",
       " 'Cyprinus',\n",
       " 'Carassius']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.csv_processor.getCoarseList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_hierarchy(\n",
       "  (pretrained_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (custom_tl_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (9): Flatten()\n",
       "  )\n",
       "  (g_c): Sequential(\n",
       "    (linear0): Linear(in_features=200, out_features=8, bias=True)\n",
       "    (bnorm0): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU()\n",
       "  )\n",
       "  (h_y): Sequential(\n",
       "    (linear0): Linear(in_features=512, out_features=200, bias=True)\n",
       "    (bnorm0): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU()\n",
       "  )\n",
       "  (g_y): Sequential(\n",
       "    (linear0): Linear(in_features=200, out_features=17, bias=True)\n",
       "    (bnorm0): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/cf5bb78c6fcc769776b2af4bf2d86d7be5835a7f9c4face14827d44f'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort through predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming images: 100%|██████████| 1703/1703 [00:21<00:00, 78.66it/s]\n"
     ]
    }
   ],
   "source": [
    "df_misclassified = pd.DataFrame(columns=['file name', 'true label', 'probability of true label', 'predicted label'])\n",
    "df_correctlyclassified = pd.DataFrame(columns=['file name', 'true label', 'probability of true label', 'predicted label'])\n",
    "\n",
    "# get probability of correct prediction and true label\n",
    "predProblist, lbllist = CNN.getLoaderPredictionProbabilities(test_loader, model, experiment_params)\n",
    "_, predlist = torch.max(predProblist, 1)\n",
    "lbllist = lbllist.reshape(lbllist.shape[0], -1)\n",
    "predProblist = predProblist.gather(1, lbllist)\n",
    "predProblist = predProblist.reshape(1, -1)\n",
    "predProblist = predProblist[0]\n",
    "\n",
    "# sort through\n",
    "predProblist, indices = torch.sort(predProblist)\n",
    "predlist = predlist[indices]\n",
    "lbllist = lbllist[indices]\n",
    "\n",
    "for i, lbl in enumerate(lbllist):\n",
    "    prd = predlist[i]\n",
    "    prdProb = predProblist[i]\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        lbl = lbl.cpu()\n",
    "        prd = prd.cpu()\n",
    "        prdProb = prdProb.cpu()\n",
    "\n",
    "    s = dataset[i]\n",
    "    row = {'file name' : s['fileName'] , \n",
    "           'true label' : int(lbl.numpy()), \n",
    "           'probability of true label': float(prdProb.numpy()),\n",
    "           'predicted label' : int(prd.numpy())}\n",
    "    \n",
    "    if(lbl != prd):\n",
    "        df_misclassified = df_misclassified.append(row, ignore_index=True)\n",
    "    else:\n",
    "        df_correctlyclassified = df_correctlyclassified.append(row, ignore_index=True)\n",
    "        \n",
    "df_misclassified = df_misclassified.sort_values(by=[ 'true label', 'probability of true label'])\n",
    "df_correctlyclassified = df_correctlyclassified.sort_values(by=['true label', 'probability of true label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_correctlyclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display and save mispredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               file name true label  probability of true label predicted label\n",
      "41   INHS_FISH_33867.jpg          0                   0.133759              12\n",
      "45  INHS_FISH_108440.jpg          0                   0.172629              14\n",
      "50    INHS_FISH_9769.jpg          0                   0.213458               2\n",
      "23   INHS_FISH_97116.jpg          1                   0.040090               5\n",
      "2    INHS_FISH_28594.jpg          3                   0.001444              12\n",
      "..                   ...        ...                        ...             ...\n",
      "52   INHS_FISH_99134.jpg         13                   0.242304               8\n",
      "3    INHS_FISH_99505.jpg         14                   0.002454               0\n",
      "4    INHS_FISH_20964.jpg         14                   0.002781               0\n",
      "33   INHS_FISH_98989.jpg         14                   0.080536              12\n",
      "35   INHS_FISH_98093.jpg         14                   0.087230               0\n",
      "\n",
      "[63 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_misclassified)\n",
    "df_misclassified.to_csv(os.path.join(experimentPathAndName, modelName, 'misclassified examples.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and display correctly predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filename of correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               file name true label  probability of true label predicted label\n",
      "36   INHS_FISH_99119.jpg          0                   0.535152               0\n",
      "60   INHS_FISH_12001.jpg          0                   0.700548               0\n",
      "74   INHS_FISH_99816.jpg          0                   0.780972               0\n",
      "86    INHS_FISH_9871.jpg          0                   0.835673               0\n",
      "100  INHS_FISH_64283.jpg          0                   0.876908               0\n",
      "..                   ...        ...                        ...             ...\n",
      "96   INHS_FISH_64639.jpg         15                   0.863661              15\n",
      "17   INHS_FISH_97576.jpg         16                   0.393870              16\n",
      "31   INHS_FISH_97062.jpg         16                   0.477894              16\n",
      "34   INHS_FISH_62299.jpg         16                   0.522556              16\n",
      "43   INHS_FISH_99274.jpg         16                   0.593848              16\n",
      "\n",
      "[278 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_correctlyclassified)\n",
    "df_correctlyclassified.to_csv(os.path.join(experimentPathAndName, modelName, 'correctly classified examples.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left join against scores\n",
    "meta=pd.read_csv(\"../../data/INHS_corrected_2020.06.09.csv\")\n",
    "\n",
    "#annotated webform data\n",
    "qual=pd.read_csv(\"../../data/hdrwebform_20200728.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual=qual.drop(columns=[\"fixed_scientific_name\",\"scientific_name\",\"resolution\"]) #all NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "moige=qual.set_index('image_name').join(meta.set_index('fileName')).join(df_correctlyclassified.set_index('file name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correctlyclassified.set_index('file name').join(qual.set_index('image_name'),how='inner')\n",
    "df_misclassified.set_index('file name').join(qual.set_index('image_name'),how='inner')\n",
    "df_correctlyclassified['classified']='correct'\n",
    "df_misclassified['classified']='misclassified'\n",
    "df_bothclassified=df_correctlyclassified.append(df_misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bothclassified\n",
    "moige=df_bothclassified.set_index('file name').join(qual.set_index('image_name')).join(meta.set_index('fileName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Noturus flavus', 'Notropis atherinoides', 'Lepomis cyanellus',\n",
       "       'Phenacobius mirabilis', 'Lepomis humilis', 'Esox americanus',\n",
       "       'Gambusia affinis', 'Lepomis macrochirus', 'Lepomis megalotis',\n",
       "       'Lepomis gibbosus', 'Notropis stramineus', 'Notropis buccata',\n",
       "       'Notropis dorsalis', 'Notropis blennius', 'Cyprinus carpio',\n",
       "       'Notropis hudsonius', 'Noturus gyrinus'], dtype=object)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moige['scientificName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>classified</th>\n",
       "      <th>correct</th>\n",
       "      <th>misclassified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>probability of true label</th>\n",
       "      <td>0.837085</td>\n",
       "      <td>0.105787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_ruler</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_colorbar</th>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_overlapping</th>\n",
       "      <td>0.104317</td>\n",
       "      <td>0.126984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_curved</th>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.063492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_missing_parts</th>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_blur</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_color_issue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specimen_angled</th>\n",
       "      <td>8.830935</td>\n",
       "      <td>8.841270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_background_uniform</th>\n",
       "      <td>0.615108</td>\n",
       "      <td>0.698413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_parts_visible</th>\n",
       "      <td>0.866906</td>\n",
       "      <td>0.841270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_quality</th>\n",
       "      <td>8.230216</td>\n",
       "      <td>8.079365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if_fins_folded</th>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "classified                  correct  misclassified\n",
       "probability of true label  0.837085       0.105787\n",
       "if_ruler                   1.000000       1.000000\n",
       "if_colorbar                0.003597       0.000000\n",
       "if_label                   1.000000       1.000000\n",
       "if_overlapping             0.104317       0.126984\n",
       "if_curved                  0.017986       0.063492\n",
       "if_missing_parts           0.010791       0.015873\n",
       "if_blur                    0.000000       0.000000\n",
       "if_color_issue             0.000000       0.000000\n",
       "specimen_angled            8.830935       8.841270\n",
       "if_background_uniform      0.615108       0.698413\n",
       "if_parts_visible           0.866906       0.841270\n",
       "image_quality              8.230216       8.079365\n",
       "if_fins_folded             0.046512       0.025641"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plotnine import *\n",
    "moige=moige.rename(columns={'if_bent':'if_curved'})\n",
    "res=moige.groupby('classified').mean().drop(['height','width','fish_number','if_fish','fixedCatalogNumber','checked_flag','landmark_failures_number','if_label_catalog_number_correct','if_label_name_correct','if_each_fish_label'],axis=1)\n",
    "\n",
    "for incol in [col for col in moige if col.startswith('if')]:\n",
    "    moige[incol] = moige[incol].astype(float)\n",
    "flip_ifs = ['if_blur']\n",
    "for col in flip_ifs:\n",
    "    res[col] = 1-res[col]\n",
    "res.transpose()\n",
    "#ggplot(moige,aes('classified','if_fish'))+geom_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X=moige[['if_curved','if_missing_parts','image_quality','if_overlapping','if_background_uniform','if_parts_visible']]\n",
    "y=(moige['classified']=='correct').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>if_curved</th>\n",
       "      <th>if_missing_parts</th>\n",
       "      <th>if_background_uniform</th>\n",
       "      <th>if_parts_visible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_101846.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_102196.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_106829.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_107545.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_108440.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_99862.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_99867.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_99878.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_99922.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INHS_FISH_99935.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>341 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      if_curved  if_missing_parts  if_background_uniform  \\\n",
       "INHS_FISH_101846.jpg          0                 0                      0   \n",
       "INHS_FISH_102196.jpg          0                 0                      1   \n",
       "INHS_FISH_106829.jpg          0                 0                      0   \n",
       "INHS_FISH_107545.jpg          0                 0                      0   \n",
       "INHS_FISH_108440.jpg          0                 0                      0   \n",
       "...                         ...               ...                    ...   \n",
       "INHS_FISH_99862.jpg           0                 0                      1   \n",
       "INHS_FISH_99867.jpg           0                 0                      1   \n",
       "INHS_FISH_99878.jpg           0                 0                      1   \n",
       "INHS_FISH_99922.jpg           0                 0                      1   \n",
       "INHS_FISH_99935.jpg           0                 0                      1   \n",
       "\n",
       "                      if_parts_visible  \n",
       "INHS_FISH_101846.jpg                 1  \n",
       "INHS_FISH_102196.jpg                 1  \n",
       "INHS_FISH_106829.jpg                 1  \n",
       "INHS_FISH_107545.jpg                 1  \n",
       "INHS_FISH_108440.jpg                 1  \n",
       "...                                ...  \n",
       "INHS_FISH_99862.jpg                  1  \n",
       "INHS_FISH_99867.jpg                  1  \n",
       "INHS_FISH_99878.jpg                  0  \n",
       "INHS_FISH_99922.jpg                  0  \n",
       "INHS_FISH_99935.jpg                  1  \n",
       "\n",
       "[341 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['if_curved','if_missing_parts','if_background_uniform','if_parts_visible']].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INHS_FISH_101846.jpg    1\n",
       "INHS_FISH_102196.jpg    1\n",
       "INHS_FISH_106829.jpg    1\n",
       "INHS_FISH_107545.jpg    1\n",
       "INHS_FISH_108440.jpg    0\n",
       "                       ..\n",
       "INHS_FISH_99862.jpg     1\n",
       "INHS_FISH_99867.jpg     1\n",
       "INHS_FISH_99878.jpg     1\n",
       "INHS_FISH_99922.jpg     1\n",
       "INHS_FISH_99935.jpg     1\n",
       "Name: classified, Length: 341, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.472295\n",
      "         Iterations 6\n",
      "                           Results: Logit\n",
      "====================================================================\n",
      "Model:                 Logit             Pseudo R-squared:  0.013   \n",
      "Dependent Variable:    classified        AIC:               334.1051\n",
      "Date:                  2020-08-02 20:48  BIC:               357.0964\n",
      "No. Observations:      341               Log-Likelihood:    -161.05 \n",
      "Df Model:              5                 LL-Null:           -163.18 \n",
      "Df Residuals:          335               LLR p-value:       0.51451 \n",
      "Converged:             1.0000            Scale:             1.0000  \n",
      "No. Iterations:        6.0000                                       \n",
      "--------------------------------------------------------------------\n",
      "                       Coef.  Std.Err.    z    P>|z|   [0.025 0.975]\n",
      "--------------------------------------------------------------------\n",
      "if_curved             -1.1098   0.7075 -1.5685 0.1168 -2.4965 0.2770\n",
      "if_missing_parts       0.1668   1.1817  0.1412 0.8877 -2.1493 2.4830\n",
      "image_quality          0.2834   0.0739  3.8340 0.0001  0.1385 0.4283\n",
      "if_overlapping         0.1479   0.4512  0.3277 0.7431 -0.7365 1.0323\n",
      "if_background_uniform -0.5765   0.3248 -1.7751 0.0759 -1.2131 0.0600\n",
      "if_parts_visible      -0.5221   0.5496 -0.9499 0.3422 -1.5993 0.5552\n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_imputed=imp.fit_transform(X)\n",
    "X_imputed_df = pd.DataFrame(X_imputed, columns = X.columns, index = X.index)\n",
    "\n",
    "logit_model=sm.Logit(y,X_imputed_df)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave out image_quality as that is redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.496584\n",
      "         Iterations 6\n",
      "                           Results: Logit\n",
      "====================================================================\n",
      "Model:                 Logit             Pseudo R-squared:  -0.038  \n",
      "Dependent Variable:    classified        AIC:               348.6704\n",
      "Date:                  2020-08-02 20:48  BIC:               367.8298\n",
      "No. Observations:      341               Log-Likelihood:    -169.34 \n",
      "Df Model:              4                 LL-Null:           -163.18 \n",
      "Df Residuals:          336               LLR p-value:       1.0000  \n",
      "Converged:             1.0000            Scale:             1.0000  \n",
      "No. Iterations:        6.0000                                       \n",
      "--------------------------------------------------------------------\n",
      "                       Coef.  Std.Err.    z    P>|z|   [0.025 0.975]\n",
      "--------------------------------------------------------------------\n",
      "if_curved             -0.4206   0.7073 -0.5947 0.5521 -1.8068 0.9656\n",
      "if_missing_parts      -0.0219   1.1950 -0.0183 0.9854 -2.3641 2.3203\n",
      "if_overlapping         0.6908   0.4472  1.5449 0.1224 -0.1856 1.5673\n",
      "if_background_uniform  0.1332   0.2505  0.5320 0.5947 -0.3577 0.6242\n",
      "if_parts_visible       1.4038   0.2117  6.6322 0.0000  0.9890 1.8187\n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_no_iq=X_imputed_df.drop('image_quality',axis=1)\n",
    "logit_model=sm.Logit(y,X_no_iq)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
